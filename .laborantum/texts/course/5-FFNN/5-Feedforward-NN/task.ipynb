{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWr6cvb9pS3J"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import json_tricks\n",
    "import lovely_tensors as lt\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "answer = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Fully Connected Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will train a 2-layer FCNN to classify digits into 10 classes (0, 1, 2, ..., 9).\n",
    "\n",
    "To build such network, we will use:\n",
    "- Fully Connected NN\n",
    "- Adam Optimizer\n",
    "- MNIST dataset\n",
    "- Accuracy metrics\n",
    "- Softmax final activation loss function (but we will embed it into the loss function)\n",
    "- Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Prepare the data\n",
    "\n",
    "Firstly, prepare MNIST dataset. We will use `torchvision.datasets.MNIST` for that.\n",
    "\n",
    "You have to download train and test datasets and visualize some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MqGQWTDIpS3R"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets\n",
    "MNIST_train = torchvision.datasets.MNIST('./', train=True, download=True)\n",
    "MNIST_test = torchvision.datasets.MNIST('./', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the data is prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vv_Lz7PYpS3U"
   },
   "outputs": [],
   "source": [
    "X_train = MNIST_train.train_data\n",
    "y_train = MNIST_train.train_labels\n",
    "X_test = MNIST_test.test_data\n",
    "y_test = MNIST_test.test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hMhsAedlrQF5",
    "outputId": "ae08bd21-79ff-48da-9886-48996a178110"
   },
   "outputs": [],
   "source": [
    "answer['X_train.dtype'] = str(X_train.dtype)\n",
    "answer['y_train.dtype'] = str(y_train.dtype)\n",
    "answer['X_train.shape'] = X_train.shape\n",
    "answer['X_test.shape'] = X_test.shape\n",
    "answer['y_train.shape'] = y_train.shape\n",
    "answer['y_test.shape'] = y_test.shape\n",
    "\n",
    "print(X_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are usually trained in float values. Usually in `float32` or `float16`. In this exercise we will use `float32` precision.\n",
    "\n",
    "Convert your data to `float` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yTaVOrPvap6"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.float()\n",
    "X_test = X_test.float()\n",
    "## YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_gfupg4kpS3X",
    "outputId": "e72c3ff4-50f7-4695-d5d5-f838e7ac2667"
   },
   "outputs": [],
   "source": [
    "answer['X_train.dtype2'] = str(X_train.dtype)\n",
    "answer['X_test.dtype2'] = str(X_test.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the data that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "id": "Z1tFXMwJpS3e",
    "outputId": "e7c2778b-d6f5-4718-ea28-fc8544f0416c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0, :, :])\n",
    "plt.show()\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue, our data should be normalized. Right now we have image data that was stored in int8 (from 0 to 255), and after transforming it to float, it still is in the same range.\n",
    "\n",
    "What we want is the data that has 0 mean and 1 variance.\n",
    "\n",
    "**Task**\n",
    "Scale and center your data so that it has 0 mean and 1 variance.\n",
    "\n",
    "To calculate mean and standard deviation of colors, use only training dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train.mean()\n",
    "std = X_train.std()\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "answer['X_train.mean'] = mean.detach().numpy()\n",
    "answer['X_train.std'] = std.detach().numpy()\n",
    "\n",
    "answer['X_train_normalized.mean'] = X_train.mean().detach().numpy()\n",
    "answer['X_train_normalized.std'] = X_train.std().detach().numpy()\n",
    "\n",
    "answer['X_test_normalized.mean'] = X_test.mean().detach().numpy()\n",
    "answer['X_test_normalized.std'] = X_test.std().detach().numpy()\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should turn our data into vectors to be able to perform forward propagation.\n",
    "\n",
    "For that we will take the last two dimensions and squeeze them into one. This is done using `reshape` method.\n",
    "\n",
    "Note that after that tensor can be turned back using reshape command again.\n",
    "\n",
    "Your initial tensor has shape $N \\times 28 \\times 28$, you have to turn it to $N \\times 28^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUZgNg7zpS3j"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 28 * 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28 * 28)\n",
    "\n",
    "answer['X_train.shape2'] = X_train.shape\n",
    "answer['X_test.shape2'] = X_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Build a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create our neural network.\n",
    "\n",
    "In pytorch to enable all the magic for training, people enherit their models from `torch.nn.Module`.\n",
    "\n",
    "This enables many utilities that will be used in the future.\n",
    "\n",
    "The main two methods that we have to fill are:\n",
    "- `__init__` method that creates all modules of Neural Network\n",
    "- `__call__` method that is used to calculate preditions\n",
    "\n",
    "Here is the network that you should create:\n",
    "- it should have the structure:\n",
    "    - Linear `28 * 28 -> n_hidden_neurons`\n",
    "    - Sigmoid\n",
    "    - Linear `n_hidden_neurons -> 10`\n",
    "- the signals that you create should traverse the initialized modules according to the order above\n",
    "\n",
    "Note that in theory, we should use a SoftMax activation in the end of this network. But in our case, this activation will be joined with the loss function due to mathematical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k7l65slppS3l"
   },
   "outputs": [],
   "source": [
    "class MNISTNet(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super().__init__()\n",
    "\n",
    "        ## YOUR CODE HERE\n",
    "        self.fc1 = torch.nn.Linear(28 * 28, n_hidden_neurons)\n",
    "        self.ac1 = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 10)\n",
    "        \n",
    "    \n",
    "    def __call__(self, signal):\n",
    "\n",
    "        ## YOUR CODE HERE\n",
    "        signal = self.fc1(signal)\n",
    "        signal = self.ac1(signal)\n",
    "        signal = self.fc2(signal)\n",
    "\n",
    "\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that your network actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_net = MNISTNet(100)\n",
    "check_input = torch.randn(10, 28 * 28)\n",
    "check_output = mnist_net(check_input)\\\n",
    "\n",
    "answer['check_result'] = check_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Create loss function and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to make a loss function (Cross Entropy with SoftMax in our case) and an optimizer (`AdamW` with learning rate `lr`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00_2j2igpS3o"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "learning_rate = 1e-3\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(mnist_net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['loss'] = str(loss)\n",
    "answer['optimizer'] = str(optimizer)\n",
    "\n",
    "print(loss)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the network\n",
    "\n",
    "I have already created a small loop that shuffles the data.\n",
    "\n",
    "Your task is to create a training cycle for one epoch with the following steps:\n",
    "1. extracts training input and lables from shuffled training data, batch after batch\n",
    "2. switches the network to training state (`model.train()`)(will be important for batchnorms and some other modules)\n",
    "2. generates predictions from the training inputs\n",
    "3. calculates loss function that compares the predictions to the targets\n",
    "4. do backpropagation\n",
    "5. make optimization step  with optimizer\n",
    "6. reset optimizer's gradients\n",
    "7. switches the network to validation state (`model.eval()`)\n",
    "\n",
    "After running one epoch, you should:\n",
    "- make prediction for train and test sets\n",
    "- calculate accuracy for the predictions for both sets\n",
    "- calculate loss function for the predictions for both sets\n",
    "\n",
    "Note that:\n",
    "- it is important to drop the last deficient batch, but we do not do it in this exercise\n",
    "- it is easy to get the class using the prediction vector as it corresponds to the output value with maximal logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZtqiGvfpS3r"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "test_accuracy_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "train_accuracy_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "# X_test = X_test.to(device)\n",
    "# y_test = y_test.to(device)\n",
    "\n",
    "for epoch in trange(5):\n",
    "    order = np.random.permutation(len(X_train))\n",
    "    # ------------------------\n",
    "    for start_index in range(0, len(X_train), batch_size):\n",
    "        ## YOUR EPOCH CODE HERE\n",
    "        batch_indexes = order[start_index:start_index + batch_size]\n",
    "\n",
    "        x_batch = X_train[batch_indexes]\n",
    "        y_batch = y_train[batch_indexes]\n",
    "\n",
    "        mnist_net.train()\n",
    "\n",
    "        preds = mnist_net(x_batch)\n",
    "        batch_loss = loss(preds, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    ## YOUR EVALUATION CODE HERE\n",
    "    mnist_net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_preds = mnist_net(X_train)\n",
    "        test_preds = mnist_net(X_test)\n",
    "\n",
    "        train_pred_labels = train_preds.argmax(dim=1)\n",
    "        test_pred_labels = test_preds.argmax(dim=1)\n",
    "\n",
    "        train_accuracy = (train_pred_labels == y_train).float().mean()\n",
    "        test_accuracy = (test_pred_labels == y_test).float().mean()\n",
    "\n",
    "    # ------------------------\n",
    "    train_loss_history.append(loss(train_preds, y_train).detach().numpy())\n",
    "    train_accuracy_history.append(train_accuracy.detach().numpy())\n",
    "\n",
    "    test_loss_history.append(loss(test_preds, y_test).detach().numpy())\n",
    "    test_accuracy_history.append(test_accuracy.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLnumX3SpS3u"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_accuracy_history)\n",
    "plt.plot(test_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nu1318JpS3y"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_history)\n",
    "plt.plot(test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['train_loss_histoory'] = train_loss_history\n",
    "answer['test_loss_history'] = test_loss_history\n",
    "answer['train_acc_history'] = train_accuracy_history\n",
    "answer['test_ann_history'] = test_accuracy_history\n",
    "\n",
    "json_tricks.dump(answer, '.answer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the network deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a network and a training cycle for it. And it works!\n",
    "\n",
    "In the next exercises we will create all the modules that are required to build deep networks. We will also use batchnormalization in these modules and experiment with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Create LinAct block\n",
    "\n",
    "Create a module that contains `torch.Sequential` module with:\n",
    "- Linear layer `n_channels -> n_channels`\n",
    "- Activation function (that is set to `Sigmoid` by default)\n",
    "\n",
    "Create a module `NoResidual` that is able to wrap a block of the network. It should work just like the original module, and its purpose is that we will replace it with `Residual` block in the future.\n",
    "\n",
    "After that, create DeepMNISTNet. This network should contain:\n",
    "- A Linear adapter module to project $28 \\times 28$ pixels to `n_channels`\n",
    "- `n_layers` instances of wrappers, each wrapping 2 sequential `block`-s (with correct number of channels and correct activation function)\n",
    "- A final classifier linear module that projects the signal of `n_channels` to number of classes (10 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinAct(torch.nn.Module):\n",
    "    def __init__(self, num_channels,activation_fn=torch.nn.Sigmoid):\n",
    "      super().__init__()\n",
    "      self.linear = torch.nn.Linear(num_channels, num_channels)\n",
    "      self.activation = activation_fn()\n",
    "\n",
    "    def __call__(self, signal):\n",
    "      signal = self.linear(signal)\n",
    "      signal = self.activation(signal)\n",
    "      return signal\n",
    "\n",
    "\n",
    "class NoResidual(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        signal = self.module(signal)\n",
    "        return signal\n",
    "\n",
    "\n",
    "class DeepMNISTNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels,\n",
    "            n_layers,\n",
    "            block=LinAct,\n",
    "            wrapper=NoResidual,\n",
    "            activation_fn=torch.nn.Sigmoid):\n",
    "      \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_adapter = torch.nn.Linear(28 * 28, n_channels)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(wrapper(block(n_channels, activation_fn)))\n",
    "        self.classifier = torch.nn.Linear(n_channels, 10)\n",
    "    \n",
    "    def __call__(self, signal):\n",
    "        signal = self.linear_adapter(signal)\n",
    "        for layer in self.layers:\n",
    "            signal = layer(signal)\n",
    "        signal = self.classifier(signal)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepMNISTNet(\n",
    "    n_channels=28*28, \n",
    "    n_layers=4)\n",
    "\n",
    "test_input = torch.zeros([10, 28 * 28])\n",
    "test_output = model(test_input)\n",
    "\n",
    "answer['model_deep'] = str(model)\n",
    "answer['test_output_deep'] = test_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "\n",
    "Build a residual wrapper.\n",
    "\n",
    "This layer should wrap any specified module, and the main idea of it should be that if the module works like $\\mathbf f(\\mathbf x)$, then wrapped in residual layer, it should work like $\\mathbf f(\\mathbf x) + \\mathbf x$\n",
    "\n",
    "So, the main question here is how to implement `__call__` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        \n",
    "        signal = self.model(signal) + signal\n",
    "        \n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepMNISTNet(\n",
    "    n_channels=28*28, \n",
    "    n_layers=4,\n",
    "    wrapper=Residual\n",
    ")\n",
    "\n",
    "test_input = torch.randn([10, 28 * 28])\n",
    "test_output = model(test_input)\n",
    "\n",
    "answer['model_deep_residual'] = str(model)\n",
    "answer['test_output_deep_residual'] = test_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7\n",
    "\n",
    "Build a LinNormAct block that is similar to `LinAct`, but has a normalization module before activation function:\n",
    "- `torch.nn.Linear`\n",
    "- `torch.nn.BatchNorm1d`\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinNormAct(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_channels,\n",
    "        activation_fn=torch.nn.Sigmoid):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = torch.nn.Linear(num_channels, num_channels)\n",
    "        self.norm = torch.nn.BatchNorm1d(num_channels)\n",
    "        self.activation = activation_fn()\n",
    "\n",
    "    def __call__(self, signal):\n",
    "        return signal\n",
    "\n",
    "\n",
    "model = DeepMNISTNet(\n",
    "    n_channels=28*28, \n",
    "    n_layers=4,\n",
    "    wrapper=Residual,\n",
    "    block=LinNormAct\n",
    ")\n",
    "\n",
    "test_input = torch.randn([10, 28 * 28])\n",
    "test_output = model(test_input)\n",
    "\n",
    "answer['model_deep_residual_linnormact'] = str(model)\n",
    "answer['test_output_deep_residual_linnormact'] = test_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "json_tricks.dump(answer, '.answer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations\n",
    "\n",
    "Here, obligatory part ends. With it, we have prepared the basic script to train our neural network.\n",
    "\n",
    "Nevertheless, we can do a lot of interesting experiments now:\n",
    "\n",
    "- for 100 epochs check, how does the training process go\n",
    "- check, how does the training process go for `n_layers = 1, 2, 4, 8, 16` \n",
    "- for `n_layers=16`, check, whether the network trains with and without residual connections\n",
    "- for the network with `n_layers=16`, add batch normalization layer, and check how the network trains with and without batch normalization\n",
    "- additionally, you can make bottleneck blocks out of residual blocks. Check, how much more are they efficient and how much does the quality drop when the size of internal representation is squeezed twice: `N -> N/2 -> N`\n",
    "- for the best network, add learning rate scheduling such as reduce LR on Plateau\n",
    "- for the best network, use AdamW with LookAhead (can be found as a separate package)\n",
    "\n",
    "To run these experiments, just take your training procedure that we have created, and substitute the model with a model of the interest.\n",
    "\n",
    "With such a simple networks, the experiments potentially can be run on the local machine, but I would recommend uploading this notebook to `Google Colab` and using a GPU (it may cost around 5$/month)\n",
    "\n",
    "Note that to use `CUDA` for your experiments, you have to\n",
    "- move model to `cuda` device using `.cuda()` or `.to()` methods\n",
    "- move batch tensors to `cuda` device the same way\n",
    "\n",
    "To report your results, you can use matplotlib, but there is a better options for machine learning. There are logging systems for ML that are called:\n",
    "- tensorboard\n",
    "- mlflow\n",
    "- weights and biases\n",
    "- neptune ai\n",
    "- and others\n",
    "\n",
    "I would recommend using the first two and maybe the third one. Tensorboard is a powerful tool to visualize the progress of your training process, while mlflow is a nice tool for journaling the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your solutions submitting your results here:\n",
    "\n",
    "https://www.kaggle.com/competitions/digit-recognizer"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson 5 Digits Recognition Video.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
