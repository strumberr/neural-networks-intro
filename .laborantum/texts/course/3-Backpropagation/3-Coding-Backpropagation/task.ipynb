{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json_tricks\n",
                "import numpy\n",
                "\n",
                "\n",
                "inputs1 = json_tricks.load(open('inputs1.json'))\n",
                "inputs2 = json_tricks.load(open('inputs2.json'))\n",
                "\n",
                "answer = {}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implementing Backpropagation package for Numpy"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this project, we will implement the backpropagation algorithm for numpy package.\n",
                "Your task will be to implement all the methods of the Node class that will be a wrapper for numpy arrays\n",
                "supporting backpropagation.\n",
                "\n",
                "Step-by-step, we will implement the following methods:\n",
                "\n",
                "1. `__init__` (a constructor)\n",
                "3. `backward` (a recursive mechanism that triggers backpropagation)\n",
                "4. `__neg__` (negation operator $- x$)\n",
                "5. `__add__` (addition operator $x + y$)\n",
                "6. `__mul__` (product operator $x \\cdot y$)\n",
                "7. `__sub__` (substitution operator $x - y$)\n",
                "8. `__truediv__` (division operator $x / y$)\n",
                "9. `exp` (exponentiation $\\exp(x)$)\n",
                "10. `sum` (summation $\\sum_{k} x_k$)\n",
                "11. `matmul` (matrix product $XY$)\n",
                "\n",
                "After that we will use the implemented methods on:\n",
                "1. Simple graph from the previous task\n",
                "2. Two-layer neural network\n",
                "\n",
                "Let's start!"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1\n",
                "\n",
                "Implement the `__init__` method for the `Node` class:\n",
                "- create field `data` and assign it to the `data` argument (we will need it to compute the gradient)\n",
                "- create field `grad` and assign it to the `None` (it will be used to store the gradient of the node)\n",
                "- create field `inputs` and assign it to the empty list (it will be used to store the nodes on which the current node depends)\n",
                "- create field `n_dependents` and assign it to the 0 (it will be used to count the number of nodes that depend on the current node)\n",
                "- create field `n_grads` and assign it to the 0 (it will be used to count the number of gradients that are coming to the current node)\n",
                "\n",
                "Implement the method `update_grad` for the `Node` class. The point of this method is to take care of the gradients that are coming from the next nodes and accumulate them:\n",
                "- firstly, create a deep copy of `grad_output` (just in case): `grad_output = copy.deepcopy(grad_output)`\n",
                "- increase the `n_grads` counter by 1 as one of the gradients of the dependents is registered\n",
                "- if `grad_output` is `None` and the size of `data` is 1, assign `grad_output` to the numpy array `[1]` (as we can calculate derivatives only of scalar values without any dependencies)\n",
                "- if `grad_output` is `None`, raise the `BaseException` with the message \"Backpropagation is impossible\" (as we can't compute the gradient without the gradient from the next node except for the case of a scalar value)\n",
                "- if `self.grad` is `None`, assign `grad_output` to the `self.grad` (that means that if there was no gradient before, we will just assign the one that we got from the next node)\n",
                "- otherwise, add `grad_output` to the `self.grad` (that means that if there was a gradient before, we will add the one that we got from the next node to the accumulated value)\n",
                "\n",
                "Implement the `backward` method for the `Node` class. This method is the heart of the backpropagation algorithm. And it logic is the following:\n",
                "- it waits and collects gradients from all the dependents of the current node\n",
                "- then it calculates the list of gradients for the inputs of the current node using the `backward_step` method (this method we will implement for every operation by hand)\n",
                "- triggers the `backward` method of the inputs with the calculated gradient in the current node as an argument\n",
                "\n",
                "As a formula, it looks like this:\n",
                "$\\partial_w {L} = \\sum_{k=1}^{n} \\partial_w {z_k} \\partial_{z_k} {L}$\n",
                "\n",
                "So, how this works in practice?\n",
                "- call the `update_grad` method with the `grad_output` argument\n",
                "- if we have collected all the gradients from the dependents, it is time to calculate the gradients for the inputs of the current node \n",
                "- call the `backward_step` method with the `grad_output` argument to calculate the gradients for the inputs\n",
                "- call the `backward` method of the inputs with the calculated gradient in the current node as an argument\n",
                "- reset the `n_grads` counter to 0 (actually, this is not necessary as we do backpropagation only once)\n",
                "\n",
                "Also `backward_step` method should be implemented for basic node too. It should return an empty list as basic node does not have any inputs.\n",
                "\n",
                "In general, we will sotre inputs to the operation as list of nodes in the `inputs` field. The `backward_step` function should return the corresponding list of gradients for the inputs.\n",
                "\n",
                "After these steps are done, you can already check that the first test passes successfully\n",
                "\n",
                "# Task 2\n",
                "\n",
                "Implement the `__add__` method for the `Node` class and the class `TensorSum` that inherits from `Node`. \n",
                "This method enables to use the `+` operator for a pair of objects of `Node` class and means that after we perofrm `z = x + y`, result\n",
                "of this operation is also a Node that correspond to the sum of two nodes.\n",
                "\n",
                "The engine for that operation is `TensorSum` class, here you need to:\n",
                "- initialize the `Node` class with the `None` argument (empty envelope of the node) in the `__init__` method\n",
                "- in the `__call__` method (this method is used when we perform `z = x + y`):\n",
                "    - assign the `inputs` fields of the current node to the `[input1, input2]` to store the inputs\n",
                "    - set `n_dependents` to the sum of `n_dependents` of the inputs (or simply to 2 as we have only 2 inputs)\n",
                "    - calculate value of `data` field of the current node using the `self.inputs[0].data + self.inputs[1].data` formula\n",
                "    - set the `grad` field of the current node to `None` as we don't have any gradient yet\n",
                "    - return the `self` node (so that the result of the operation is the current node)\n",
                "- in the `backward_step` method:\n",
                "    - return the list of gradients for the inputs: `[grad_output, grad_output]` as $\\partial_x {x + y} = 1$ and $\\partial_y {x + y} = 1$\n",
                "\n",
                "\n",
                "In `Node` class:\n",
                "- implement the `__add__` method:\n",
                "    - create a new `TensorSum` object with the sum of the `data` of the current node and the `other` node and return it (`return TensorSum()(self, other)`)\n",
                "\n",
                "Now your Node class supports the `+` operator. You can check it by runnint test number 2. So you can calculate gradients of expressions like `x + y + x + x + y + y`. That is no much, but that is a good start.\n",
                "\n",
                "# Task 3 and others\n",
                "\n",
                "Implement the remaining operations by analogy:\n",
                "- `__sub__` that will be used when we perform `z = x - y`\n",
                "- `__mul__` that will be used when we perform `z = x * y`\n",
                "- `__truediv__` that will be used when we perform `z = x / y`\n",
                "- `__neg__` that will be used when we perform `z = -x`\n",
                "- `exp` that will be used when we perform `z = numpy.exp(x)`\n",
                "- `sum` that will be used when we perform `z = numpy.sum(x)`\n",
                "- `matmul` that will be used when we perform `z = x @ y`\n",
                "\n",
                "In all these cases, you should use theoretircal derivatives to implement `backward_step` function for every of these operations.\n",
                "\n",
                "The hardest one will be `matmul` as it is a matrix multiplication. To implement it, use the following rule of thumb:\n",
                "- the resulting gradient should have the same shape as data\n",
                "- the gradients of linear functions are also some linear functions"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy\n",
                "import copy\n",
                "\n",
                "class Node:\n",
                "    def __init__(self, data):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def update_grad(self, grad_output):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def backward(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    # @backprop\n",
                "    # def backward(self, grad_output=None):\n",
                "\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Node(data={self.data}, grad={self.grad})\"\n",
                "\n",
                "    def __neg__(self):\n",
                "        return TensorNeg()(self)\n",
                "\n",
                "    def __add__(self, other):\n",
                "        return TensorSum()(self, other)\n",
                "\n",
                "    def __mul__(self, other):\n",
                "        return TensorMul()(self, other)\n",
                "\n",
                "    def __sub__(self, other):\n",
                "        return TensorSub()(self, other)\n",
                "\n",
                "    def __truediv__(self, other):\n",
                "        return TensorDiv()(self, other)\n",
                "\n",
                "    def exp(self):\n",
                "        return TensorExp()(self)\n",
                "\n",
                "    def sum(self, axis=None):\n",
                "        return TensorSumReduce()(self)\n",
                "\n",
                "    def __matmul__(self, other):\n",
                "        return TensorMatMul()(self, other)\n",
                "\n",
                "\n",
                "class TensorSum(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "    \n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "\n",
                "class TensorSub(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "\n",
                "class TensorMul(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "\n",
                "class TensorDiv(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "\n",
                "class TensorNeg(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "\n",
                "class TensorExp(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "\n",
                "class TensorSumReduce(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "class TensorMatMul(Node):\n",
                "    def __init__(self):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "\n",
                "    def __call__(self, input1, input2):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n",
                "        return self\n",
                "\n",
                "    def backward_step(self, grad_output=None):\n",
                "        ...\n",
                "        ### YOUR CODE HERE ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 1\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "print(x)\n",
                "x.backward()\n",
                "print(x)\n",
                "\n",
                "answer['init'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "\n",
                "    x.backward()\n",
                "\n",
                "    answer['init'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 2\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = x + y + x + x + x + y\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['sum'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = x + y + x + x + x + y\n",
                "    z.backward()\n",
                "\n",
                "    answer['sum'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 3\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = x - y + x + x - x - y\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['diff'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = x - y + x + x - y - y\n",
                "    z.backward()\n",
                "\n",
                "    answer['diff'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 4\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = (x + y) * (x - y)\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['mul'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = (x + y) * (x - y) * (x + x + y)\n",
                "    z.backward()\n",
                "\n",
                "    answer['mul'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 5\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x, y)\n",
                "\n",
                "answer['div'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "    y = Node(inp['x'][1])\n",
                "\n",
                "    z = x / (Node(0.5) + y)\n",
                "    z.backward()\n",
                "\n",
                "    answer['div'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 6\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = -x\n",
                "\n",
                "print(z)\n",
                "z.backward()\n",
                "print(x)\n",
                "\n",
                "answer['neg'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "\n",
                "    z = -x\n",
                "    z.backward()\n",
                "\n",
                "    answer['neg'].append(x.grad)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 7\n",
                "\n",
                "x = Node(numpy.array([1]))\n",
                "y = Node(numpy.array([2]))\n",
                "\n",
                "z = (x + y).exp()\n",
                "\n",
                "print(z)\n",
                "print(x.grad, y.grad)\n",
                "\n",
                "z.backward()\n",
                "\n",
                "print(x.grad, y.grad)\n",
                "\n",
                "answer['exp'] = []\n",
                "for inp in inputs1:\n",
                "    x = Node(inp['x'][0])\n",
                "\n",
                "    z = x.exp()\n",
                "    z.backward()\n",
                "\n",
                "    answer['exp'].append(x.grad)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task\n",
                "\n",
                "Implement Graph function from the previous task. For the constants please use `Node(1)` whenever needed (otherwise you will get an error)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 8 (Graph)\n",
                "\n",
                "def sigmoid(z):\n",
                "    ...\n",
                "    ### YOUR CODE HERE ###\n",
                "\n",
                "def tanh(z):\n",
                "    ...\n",
                "    ### YOUR CODE HERE ###\n",
                "\n",
                "def graph_value(x, w):\n",
                "    y = x\n",
                "    ## YOUR CODE HERE ##\n",
                "    return y\n",
                "\n",
                "answer['graph'] = []\n",
                "for inp in inputs1:\n",
                "\n",
                "    x = inp['x']\n",
                "    w = inp['w']\n",
                "\n",
                "    x = [Node(float(val)) for val in x]\n",
                "    w = [Node(float(val)) for val in w]\n",
                "\n",
                "    y = graph_value(x, w)\n",
                "    y.backward()\n",
                "\n",
                "    answer['graph'].append([x[0].grad, x[1].grad, w[0].grad, w[1].grad, w[2].grad, w[3].grad])\n",
                "\n",
                "print(\"OUR RESULTS:\")\n",
                "print(x[0].grad, x[1].grad, w[0].grad, w[1].grad, w[2].grad, w[3].grad)\n",
                "\n",
                "def torch_graph_value(x, w):\n",
                "    y = x\n",
                "    ## YOUR CODE HERE ##\n",
                "    return y\n",
                "\n",
                "import torch\n",
                "x = torch.tensor(inp['x'], requires_grad=True, dtype=float)\n",
                "w = torch.tensor(inp['w'], requires_grad=True, dtype=float)\n",
                "\n",
                "y = torch_graph_value(x, w)\n",
                "y.backward()\n",
                "\n",
                "print(\"TORCH RESULTS:\")\n",
                "print(x.grad, w.grad)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2-layer NN\n",
                "\n",
                "Implement 2 layer Neural Network and compute its gradient using `Node` class:\n",
                "\n",
                "$\\mathbf y = \\sigma( W_2 \\sigma(W_1 \\mathbf x + \\mathbf b_1) + \\mathbf b_2)$\n",
                "\n",
                "Return sum of all values in $y * y$ as loss function"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TEST 9 (Two-layer net)\n",
                "\n",
                "def two_layer_net(x, W1, W2, b1, b2):\n",
                "    ...\n",
                "    ## YOUR CODE HERE ##\n",
                "    return y.sum()\n",
                "\n",
                "\n",
                "answer['two_layer_net'] = []\n",
                "for inp in inputs2:\n",
                "    x = Node(inp['x'])\n",
                "    W1 = Node(inp['W1'])\n",
                "    W2 = Node(inp['W2'])\n",
                "    b1 = Node(inp['b1'])\n",
                "    b2 = Node(inp['b2'])\n",
                "\n",
                "    h_hat = two_layer_net(x, W1, W2, b1, b2)\n",
                "    h_hat.backward()\n",
                "\n",
                "    answer['two_layer_net'].append([x.grad, W1.grad, W2.grad, b1.grad, b2.grad])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusion\n",
                "\n",
                "You have implemented a backpropagation algorithm. This algorithm is similar to one that is used in Torch. Note that you have implemented all the mechanics of it. Thus it should be now not a magical box: you know exactly how it works."
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_tricks.dump(answer, '.answer.json')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}